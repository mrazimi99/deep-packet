{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepPacket.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1uAWlcaY6JE_geOUfJHVLbWTWfpV7SyfE",
      "authorship_tag": "ABX9TyOjES1bW4jRWzVm6OnWFFt5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrazimi99/deep-packet/blob/main/DeepPacket.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_DUBMHZuVXh"
      },
      "source": [
        "!pip install scapy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FVOGPeeOnT5",
        "outputId": "59ec96e6-39b2-4ee7-c383-f2d9befddc91"
      },
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import os.path\n",
        "import click\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from joblib import Parallel, delayed\n",
        "from scapy.compat import raw\n",
        "from scapy.layers.inet import IP, UDP\n",
        "from scapy.layers.l2 import Ether\n",
        "from scapy.packet import Padding\n",
        "from scapy.utils import rdpcap\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from random import sample\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import time\n",
        "from datetime import timedelta\n",
        "from datetime import datetime\n",
        "\n",
        "from utils import should_omit_packet, PREFIX_TO_APP_ID, PREFIX_TO_TRAFFIC_ID"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcBZoPxz7oix"
      },
      "source": [
        "def remove_ether_header(packet):\n",
        "    if Ether in packet:\n",
        "        return packet[Ether].payload\n",
        "\n",
        "    return packet\n",
        "\n",
        "\n",
        "def mask_ip(packet):\n",
        "    if IP in packet:\n",
        "        packet[IP].src = '0.0.0.0'\n",
        "        packet[IP].dst = '0.0.0.0'\n",
        "\n",
        "    return packet\n",
        "\n",
        "\n",
        "def pad_udp(packet):\n",
        "    if UDP in packet:\n",
        "        # get layers after udp\n",
        "        layer_after = packet[UDP].payload.copy()\n",
        "\n",
        "        # build a padding layer\n",
        "        pad = Padding()\n",
        "        pad.load = '\\x00' * 12\n",
        "\n",
        "        layer_before = packet.copy()\n",
        "        layer_before[UDP].remove_payload()\n",
        "        packet = layer_before / pad / layer_after\n",
        "\n",
        "        return packet\n",
        "\n",
        "    return packet\n",
        "\n",
        "\n",
        "def packet_to_sparse_array(packet, max_length=1500):\n",
        "    arr = np.frombuffer(raw(packet), dtype=np.uint8)[0: max_length] / 255\n",
        "    if len(arr) < max_length:\n",
        "        pad_width = max_length - len(arr)\n",
        "        arr = np.pad(arr, pad_width=(0, pad_width), constant_values=0)\n",
        "\n",
        "    arr = sparse.csr_matrix(arr)\n",
        "    return arr\n",
        "\n",
        "\n",
        "def transform_packet(packet):\n",
        "    if should_omit_packet(packet):\n",
        "        return None\n",
        "\n",
        "    packet = remove_ether_header(packet)\n",
        "    packet = pad_udp(packet)\n",
        "    packet = mask_ip(packet)\n",
        "\n",
        "    arr = packet_to_sparse_array(packet)\n",
        "\n",
        "    return arr\n",
        "\n",
        "pcaps = 'drive/MyDrive/Pcaps/'\n",
        "dataset = []\n",
        "\n",
        "for path in os.listdir(pcaps):\n",
        "    print('Processing ' + path)\n",
        "    cnt = 0\n",
        "    for i, packet in enumerate(rdpcap(pcaps + path)):\n",
        "        #####\n",
        "        if cnt > 100:\n",
        "            break\n",
        "        #####\n",
        "        arr = transform_packet(packet)\n",
        "        if arr is not None:\n",
        "            cnt += 1\n",
        "            # get labels for app identification\n",
        "            prefix = path.split('.')[0].lower()\n",
        "            app_label = PREFIX_TO_APP_ID.get(prefix)\n",
        "            traffic_label = PREFIX_TO_TRAFFIC_ID.get(prefix)\n",
        "            row = {\n",
        "                'app_label': app_label,\n",
        "                'traffic_label': traffic_label,\n",
        "                'feature': torch.from_numpy(np.array(arr.todense().tolist()[0], dtype=np.float32))\n",
        "            }\n",
        "            dataset.append(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv7-3usPJ3u9"
      },
      "source": [
        "traffic_labels = [row['traffic_label'] for row in dataset]\n",
        "app_labels = [row['app_label'] for row in dataset]\n",
        "traffic_classes = set(traffic_labels)\n",
        "app_classes = set(app_labels)\n",
        "\n",
        "traffic_train_all, traffic_test_all = train_test_split(dataset, test_size=0.2, random_state=42, stratify=traffic_labels)\n",
        "app_train_all, app_test_all = train_test_split(dataset, test_size=0.2, random_state=42, stratify=app_labels)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBA3SdIiLwur"
      },
      "source": [
        "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
        "traffic_features = [elem['feature'] for elem in traffic_train_all]\n",
        "train_traffic_labels = [elem['traffic_label'] for elem in traffic_train_all]\n",
        "traffic_train, traffic_y = undersample.fit_resample(traffic_features, train_traffic_labels)\n",
        "\n",
        "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
        "app_features = [elem['feature'] for elem in app_train_all]\n",
        "train_app_labels = [elem['app_label'] for elem in app_train_all]\n",
        "app_train, app_y = undersample.fit_resample(app_features, train_app_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krD6bp6Fp_UB"
      },
      "source": [
        "traffic_train_final = [{'feature': f.tolist(), 'label': l} for f, l in zip(traffic_train, traffic_y)]\n",
        "app_train_final = [{'feature': f.tolist(), 'label': l} for f, l in zip(app_train, app_y)]"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoG1Qy1NQ95i"
      },
      "source": [
        "batch_size = 64\n",
        "num_workers = 2\n",
        "traffic_train_loader = torch.utils.data.DataLoader(traffic_train_final, batch_size=batch_size, num_workers=num_workers)\n",
        "app_train_loader = torch.utils.data.DataLoader(app_train_final, batch_size=batch_size, num_workers=num_workers)\n",
        "traffic_test_loader = torch.utils.data.DataLoader(traffic_test_all, batch_size=batch_size, num_workers=num_workers)\n",
        "app_test_loader = torch.utils.data.DataLoader(app_test_all, batch_size=batch_size, num_workers=num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmato3xNRhxT"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_num, middle_layers, output_num, act):\n",
        "        super(Model, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.acts = []\n",
        "\n",
        "        self.layers.append(nn.Linear(input_num, middle_layers[0]))\n",
        "        self.acts.append(act)\n",
        "\n",
        "        for i in range(len(middle_layers) - 1):\n",
        "            self.layers.append(nn.Linear(middle_layers[i], middle_layers[i + 1]))\n",
        "            self.acts.append(act)\n",
        "\n",
        "        self.layers.append(nn.Linear(middle_layers[-1], output_num))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            x = self.layers[i](x)\n",
        "            x = self.acts[i](x)\n",
        "\n",
        "        x = self.layers[-1](x)\n",
        "        return x"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_9ZsVWjRiwS",
        "outputId": "eb06f9e5-4047-4b53-c1a5-7842cb0dbf57"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ucN-6xXRswA"
      },
      "source": [
        "def fit(model, train_loader, device, criterion, optimizer, num_epochs=10):\n",
        "    total_time = 0.\n",
        "    loss_per_epoch = {e + 1 : 0 for e in range(num_epochs)}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0.\n",
        "        d1 = datetime.now()\n",
        "\n",
        "        for elem in torch.TensorData(train_loader):\n",
        "            byte_array = elem['feature'].to(device)\n",
        "            labels = elem['traffic_label'].to(device)\n",
        "\n",
        "            # Clear gradients w.r.t. parameters\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass to get output/logits\n",
        "            outputs = model(byte_array)\n",
        "\n",
        "            # Calculate Loss: softmax --> cross entropy loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Getting gradients w.r.t. parameters\n",
        "            loss.backward()\n",
        "\n",
        "            # Updating parameters\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        average_loss = train_loss / len(train_loader)\n",
        "        d2 = datetime.now()\n",
        "        delta = d2 - d1\n",
        "        seconds = float(delta.total_seconds())\n",
        "        total_time += seconds\n",
        "        loss_per_epoch[epoch + 1] = average_loss\n",
        "        print('epoch %d, train_loss: %.3f, time elapsed: %s seconds' % (epoch + 1, average_loss, seconds))\n",
        "\n",
        "    if math.isnan(max(loss_per_epoch.values())):\n",
        "        print('Could not plot for nan losses.')\n",
        "    else:\n",
        "        plt.plot(list(loss_per_epoch.keys()), list(loss_per_epoch.values()))\n",
        "        plt.title('Avreage Loss Per Epoch')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Aerage Loss')\n",
        "        plt.xlim(1, num_epochs + 1)\n",
        "        plt.ylim(0, max(loss_per_epoch.values()) + 1)\n",
        "        plt.show()\n",
        "\n",
        "    print('total training time: %.3f minutes' % (total_time / 60))"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK-SAGbxRvsv"
      },
      "source": [
        "def test_model_accuracy(model, task, test_loader, classes):\n",
        "    # Calculate Accuracy         \n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "    correct_predicted = {id : 0 for id in range(len(classes))}\n",
        "    num_of_labels = {id : 0 for id in range(len(classes))}\n",
        "\n",
        "    # Iterate through test dataset\n",
        "    with torch.no_grad():\n",
        "      for byte_array, labels in train_loader:\n",
        "        outputs = model(byte_array.to(device))\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "\n",
        "        for p, l in zip(predicted, labels):\n",
        "            num_of_labels[l.item()] += 1\n",
        "\n",
        "            if p == l:\n",
        "                correct_predicted[p.to('cpu').item()] += 1\n",
        "\n",
        "        correct += (predicted.to('cpu') == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print('Accuracy: {}%'.format(accuracy))\n",
        "    return [x / y for x, y in zip(correct_predicted.values(), num_of_labels.values())]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZWdvJNsZn85"
      },
      "source": [
        "def analyse(model, train_loader, test_loader, classes):\n",
        "    print('Train ', end='')\n",
        "    train_accs = test_model_accuracy(model, train_loader, classes)\n",
        "    print('Test ', end='')\n",
        "    test_accs = test_model_accuracy(model, test_loader, classes)\n",
        "    plt.figure(figsize=(30, 9), dpi= 80, facecolor='w', edgecolor='k')\n",
        "    plt.bar([i - 0.1 for i in range(len(classes))], train_accs, width=0.2, label='Train')\n",
        "    plt.bar([i + 0.1 for i in range(len(classes))], test_accs, width=0.2, label='Test')\n",
        "    plt.legend()\n",
        "    plt.xlim(-0.2, len(classes))\n",
        "    plt.xticks(list(range(len(classes))), classes, rotation='vertical')\n",
        "    plt.title('Comparision Between Test and Train Accuracies')\n",
        "    plt.xlabel('Category')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.show()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEFHAeOgZzwB"
      },
      "source": [
        "input_vector = 1500\n",
        "model = Model(input_vector, [200, 200] , len(traffic_classes), F.relu)\n",
        "model = model.to(device)\n",
        "\n",
        "learning_rate = 0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgrSnLt6s_Us"
      },
      "source": [
        "fit(model, traffic_train_loader, device, criterion, optimizer, num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjcN1DvUfvEC"
      },
      "source": [
        "analyse(model, traffic_train_loader, test_loader, classes)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}